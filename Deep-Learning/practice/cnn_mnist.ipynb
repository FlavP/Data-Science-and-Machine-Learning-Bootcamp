{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_model_fn(features, labels, mode):\n",
    "    # - 1 is to autotune bactch-size, features sunt numarul total de intrari, \n",
    "    #adica 784 * batch_size, iar in dreapta este forma\n",
    "    reshaped_inputs = tf.reshape(features, [-1, 28, 28, 1])\n",
    "    #input [batch_size, 28, 28, 1] ==> output[batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "                            inputs=reshaped_inputs,\n",
    "                            activation=tf.nn.relu,\n",
    "                            filters=32,\n",
    "                            kernel_size=[5,5],\n",
    "                            padding='same'\n",
    "                            )\n",
    "    #input [batch_size, 28, 28, 32] ==> output[batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2,2], strides=2)\n",
    "    #input [batch_size, 14, 14, 32] ==> output[batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "                            inputs=pool1,\n",
    "                            activation=tf.nn.relu,\n",
    "                            filters=64,\n",
    "                            kernel_size=[5,5],\n",
    "                            padding='same' #adica sa pastreze forma inputului, de 28 * 28\n",
    "                            )\n",
    "    #input [batch_size, 14, 14, 64] ==> output[batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\n",
    "    #Dense Layer, flat ==> tensorul redevine 2d, de marime [batch_size, output_pool2]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    #fully-connected layer de 1024 de neuroni ==> output[batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    #dropout ==> 40% din date vor fi randomly dropped in training\n",
    "    #se verifica daca modul este training, pentru a face sau nu dropout ==> output[batch_size, 1024]\n",
    "    drop = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n",
    "    #Logits sau estimare, presupun\n",
    "    logits = tf.layers.dense(inputs=drop, units=10)\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    # # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        #onehot ==> unde este valoare 1 in tensor, care declanseaza label-ul, 10 clase (depth)\n",
    "        one_hotlabels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "        #loss este scalar, valoarea returnata de softmax\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hotlabels, logits=logits)\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.contrib.framework.get_global_step(),\n",
    "        learning_rate=0.001,\n",
    "        optimizer=\"SGD\")\n",
    "    #Cauta in tensor pe orizontala argmax, adica unde este valoarea 1, cautam valoarea cea mai mare \n",
    "    #in dimensiunea cu index 1, apoi aplicam softmax pentru a obtine o probabilitate\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    return model_fn_lib.ModelFnOps(mode=mode, predictions=predictions, train_op=train_op, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    mnist = learn.datasets.load_dataset(\"mnist\")\n",
    "    train_data = mnist.train.images\n",
    "    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "    test_data = mnist.test.images\n",
    "    test_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "    mnist_estim = learn.Estimator(model_fn=conv_model_fn, model_dir=\"MNIST_data/\")\n",
    "    #Let's set up some logging to see the progress with logging_hook\n",
    "    logs = {\"probabilities\" : \"softmax_tensor\"}\n",
    "    log_hook = tf.train.LoggingTensorHook(tensors=logs, every_n_iter=50)\n",
    "    mnist_estim.fit(x=train_data, y=train_labels, batch_size=100, steps=20000, monitors=[log_hook])\n",
    "    #Accuracy metric for evaluation\n",
    "    metric = {\n",
    "        #metric_fn - functia care calculeaza si returneaza accuray, putem folosi fct default accuracy din tf.metrics\n",
    "        #prediction key, cheia care contine label-urile\n",
    "        \"accuracy\": learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"classes\")\n",
    "    }\n",
    "    #Facem evaluare\n",
    "    evalul = mnist_estim.evaluate(x=test_data, y=test_labels, metrics=metric)\n",
    "    print(evalul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_model_dir': None, '_master': '', '_task_type': None, '_environment': 'local', '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9afcca70b8>, '_task_id': 0, '_evaluation_master': '', '_is_chief': True, '_num_worker_replicas': 0}\n",
      "WARNING:tensorflow:From <ipython-input-15-4bbc3e40ac29>:11: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-15-4bbc3e40ac29>:11: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-15-4bbc3e40ac29>:11: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flavius/anaconda3/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:248: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into MNIST_data/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 2.30034\n",
      "INFO:tensorflow:probabilities = [[ 0.07334762  0.09645092  0.09379227  0.1020344   0.11243463  0.10517013\n",
      "   0.11542577  0.12032797  0.09507286  0.08594338]\n",
      " [ 0.08457483  0.09649383  0.10131261  0.10580406  0.10211761  0.10003387\n",
      "   0.11624278  0.10834341  0.09771004  0.08736698]\n",
      " [ 0.07731293  0.09835514  0.08390491  0.11773082  0.11294403  0.10046723\n",
      "   0.1118436   0.11918855  0.08111848  0.09713426]\n",
      " [ 0.07763945  0.09753578  0.0931226   0.1098754   0.12504564  0.09741591\n",
      "   0.11860139  0.10550665  0.09284841  0.08240876]\n",
      " [ 0.08334383  0.10159022  0.08611266  0.11232153  0.10084321  0.11942852\n",
      "   0.10577449  0.095222    0.11403917  0.08132433]\n",
      " [ 0.08501171  0.09069485  0.09474856  0.11539796  0.0932812   0.10224444\n",
      "   0.11005077  0.1116351   0.10274685  0.09418861]\n",
      " [ 0.08523989  0.09918077  0.08750451  0.11731132  0.09086074  0.10376801\n",
      "   0.11209524  0.11570719  0.08606778  0.10226443]\n",
      " [ 0.08586449  0.09861035  0.09038574  0.10538937  0.11140909  0.11389413\n",
      "   0.09850627  0.10022172  0.10528902  0.09042983]\n",
      " [ 0.08792929  0.10546548  0.08857651  0.11316288  0.09327154  0.11193222\n",
      "   0.10528561  0.10072067  0.08952304  0.10413282]\n",
      " [ 0.08523498  0.10973881  0.09828684  0.10313413  0.0967183   0.09291624\n",
      "   0.1066044   0.10983112  0.09874459  0.09879053]\n",
      " [ 0.08472     0.10098779  0.09944653  0.11265634  0.09162255  0.09670918\n",
      "   0.11314581  0.1176122   0.09445559  0.08864404]\n",
      " [ 0.07852314  0.09144621  0.09683081  0.09965007  0.1153627   0.09374353\n",
      "   0.11976332  0.11460327  0.10346748  0.08660957]\n",
      " [ 0.09153044  0.10859685  0.09115454  0.11562822  0.11207145  0.09959994\n",
      "   0.09311091  0.10852111  0.08653412  0.09325246]\n",
      " [ 0.09167727  0.09593726  0.09736674  0.09191201  0.10831776  0.10725237\n",
      "   0.10006122  0.11045853  0.09971831  0.09729849]\n",
      " [ 0.09493214  0.10541956  0.08643279  0.10388478  0.0961012   0.11673082\n",
      "   0.11816467  0.09288378  0.09035576  0.09509451]\n",
      " [ 0.07973021  0.10050908  0.0805401   0.11474023  0.10764834  0.09391739\n",
      "   0.11049911  0.10601299  0.10516276  0.10123983]\n",
      " [ 0.07084345  0.09273861  0.10445003  0.119894    0.10200977  0.10048319\n",
      "   0.10039517  0.10430773  0.10862367  0.09625429]\n",
      " [ 0.08720849  0.10730876  0.09627514  0.10070102  0.09363488  0.10607636\n",
      "   0.10291478  0.10901897  0.10544919  0.09141245]\n",
      " [ 0.08077846  0.10951105  0.09023459  0.10704221  0.09675662  0.10408095\n",
      "   0.10858878  0.11680195  0.09791575  0.08828963]\n",
      " [ 0.08353609  0.09940045  0.09485278  0.09947315  0.08851135  0.10932428\n",
      "   0.12090509  0.10330819  0.10563173  0.09505692]\n",
      " [ 0.09022304  0.10699587  0.09011669  0.10758333  0.10628774  0.10543028\n",
      "   0.09486337  0.10844931  0.09772035  0.09233003]\n",
      " [ 0.08397539  0.09196646  0.09991519  0.1049225   0.11675022  0.1165762\n",
      "   0.10287394  0.11238319  0.08911719  0.08151969]\n",
      " [ 0.08804333  0.10135422  0.0792058   0.11588936  0.09686239  0.10984337\n",
      "   0.10542098  0.11301356  0.09966839  0.09069852]\n",
      " [ 0.08700978  0.10027169  0.08057135  0.10707801  0.10470325  0.10914876\n",
      "   0.10981374  0.11568837  0.09571709  0.08999797]\n",
      " [ 0.0965865   0.09213176  0.1070865   0.10194731  0.10368197  0.10471176\n",
      "   0.09688495  0.10623866  0.10326207  0.08746854]\n",
      " [ 0.08502809  0.09834407  0.08907508  0.10483491  0.09928332  0.1099961\n",
      "   0.11156908  0.12047426  0.09709845  0.08429667]\n",
      " [ 0.07251131  0.0909569   0.09250531  0.11964261  0.10373434  0.10880719\n",
      "   0.10973965  0.12214933  0.08846536  0.09148802]\n",
      " [ 0.07044457  0.10337389  0.08899389  0.1093419   0.1132796   0.10829698\n",
      "   0.10702312  0.10973699  0.09487886  0.09463017]\n",
      " [ 0.08867874  0.10556524  0.09339074  0.09721481  0.09966931  0.11981195\n",
      "   0.10101099  0.10241471  0.10371367  0.08852979]\n",
      " [ 0.0789355   0.09684607  0.08207233  0.09728638  0.10956529  0.10784414\n",
      "   0.12402615  0.11553907  0.09206716  0.09581785]\n",
      " [ 0.0754351   0.10549672  0.10431968  0.11624362  0.09754769  0.10371112\n",
      "   0.10970495  0.10496952  0.0903574   0.09221422]\n",
      " [ 0.08690913  0.10532115  0.08570153  0.12158542  0.10200437  0.10065088\n",
      "   0.10133728  0.11856256  0.08901543  0.08891229]\n",
      " [ 0.08191413  0.09506324  0.09457459  0.1046191   0.10638676  0.10255519\n",
      "   0.11083097  0.12476216  0.08742411  0.09186983]\n",
      " [ 0.08048557  0.0910505   0.09966244  0.0980733   0.10893593  0.11074999\n",
      "   0.10713072  0.1160072   0.09056997  0.09733434]\n",
      " [ 0.09056598  0.0930204   0.0942563   0.12125943  0.11441877  0.10179434\n",
      "   0.0982056   0.10899697  0.09442795  0.08305425]\n",
      " [ 0.09139042  0.1065243   0.09952467  0.10013922  0.10666427  0.08963463\n",
      "   0.10615809  0.1172776   0.09890164  0.08378515]\n",
      " [ 0.08007378  0.08728091  0.08772522  0.10698037  0.11237311  0.11642165\n",
      "   0.10683931  0.10819121  0.09840995  0.09570459]\n",
      " [ 0.08849689  0.08757313  0.09169411  0.11190724  0.10376362  0.1027161\n",
      "   0.10421444  0.1117049   0.10195752  0.09597208]\n",
      " [ 0.0707666   0.09563742  0.09085362  0.11499631  0.10093372  0.0987544\n",
      "   0.10048903  0.13563953  0.10104144  0.09088793]\n",
      " [ 0.08043033  0.09460278  0.08692073  0.1111427   0.11383511  0.10192306\n",
      "   0.12515703  0.10265512  0.09593355  0.08739956]\n",
      " [ 0.07568061  0.10130571  0.09403379  0.11534832  0.10694288  0.10801519\n",
      "   0.11327918  0.10680445  0.09301206  0.08557779]\n",
      " [ 0.08531798  0.09365658  0.10412101  0.10719325  0.10126317  0.10513145\n",
      "   0.09895333  0.1243858   0.09013091  0.08984651]\n",
      " [ 0.08220966  0.10857748  0.08630423  0.11454102  0.10172832  0.09636682\n",
      "   0.11688337  0.11364686  0.10098539  0.07875682]\n",
      " [ 0.08672452  0.09931668  0.08913034  0.11238393  0.09864064  0.10542342\n",
      "   0.10009785  0.10731604  0.10190494  0.09906164]\n",
      " [ 0.06747245  0.0976907   0.09740205  0.0943993   0.0976998   0.10743616\n",
      "   0.11854611  0.136346    0.09381535  0.0891921 ]\n",
      " [ 0.07807083  0.08752315  0.09700473  0.10575557  0.11053796  0.10319927\n",
      "   0.10483432  0.11229949  0.10324041  0.09753429]\n",
      " [ 0.08632448  0.09942226  0.10050551  0.10503773  0.10019706  0.09743361\n",
      "   0.10381644  0.10549201  0.10434058  0.0974303 ]\n",
      " [ 0.08151662  0.09361131  0.09620052  0.11031972  0.100797    0.10070036\n",
      "   0.11389972  0.11049632  0.10828867  0.08416978]\n",
      " [ 0.09433337  0.10384331  0.09057948  0.10436906  0.10031517  0.10638843\n",
      "   0.101305    0.09776841  0.0931117   0.10798609]\n",
      " [ 0.08783779  0.09604638  0.09112827  0.11327152  0.10422528  0.11400324\n",
      "   0.11525957  0.10234892  0.09172516  0.08415385]\n",
      " [ 0.08695097  0.08350541  0.10151373  0.10378214  0.10550064  0.10192117\n",
      "   0.10866519  0.11465799  0.10624167  0.08726114]\n",
      " [ 0.08587775  0.11356643  0.10024926  0.09995814  0.10470015  0.10201859\n",
      "   0.10629789  0.09732664  0.09238124  0.09762388]\n",
      " [ 0.07723308  0.11667075  0.10034942  0.09544121  0.10691734  0.10127978\n",
      "   0.11571921  0.09767611  0.09989804  0.08881504]\n",
      " [ 0.09254389  0.09832686  0.08977185  0.10657307  0.10406689  0.11482415\n",
      "   0.10359451  0.10658061  0.09280155  0.09091663]\n",
      " [ 0.08587121  0.09922971  0.09873004  0.11617129  0.10179097  0.09640033\n",
      "   0.10828851  0.1156892   0.09024075  0.08758805]\n",
      " [ 0.07973     0.08902041  0.08765681  0.11042457  0.10352568  0.1058121\n",
      "   0.12132963  0.10865185  0.08756427  0.10628477]\n",
      " [ 0.08520605  0.1020117   0.09420125  0.10665501  0.10334665  0.09920504\n",
      "   0.10924864  0.11259577  0.09674967  0.09078025]\n",
      " [ 0.08199818  0.09250414  0.10079271  0.10824747  0.0983891   0.10698771\n",
      "   0.10675586  0.1058541   0.09555218  0.10291851]\n",
      " [ 0.09334523  0.10917468  0.08453337  0.1069027   0.10047706  0.10464448\n",
      "   0.10547364  0.09711859  0.09435398  0.10397626]\n",
      " [ 0.09182947  0.10369689  0.09581503  0.10776468  0.10150243  0.1028202\n",
      "   0.0996353   0.1129569   0.09881297  0.08516605]\n",
      " [ 0.08256119  0.10647595  0.0911671   0.12154721  0.10678373  0.09457532\n",
      "   0.1092312   0.11016527  0.09678221  0.08071084]\n",
      " [ 0.08636075  0.09838116  0.09516101  0.11791945  0.0920177   0.10299263\n",
      "   0.10124978  0.11000073  0.09218901  0.1037278 ]\n",
      " [ 0.09389517  0.10116063  0.0985583   0.09747601  0.11802119  0.10048493\n",
      "   0.08953656  0.10543025  0.10050663  0.09493031]\n",
      " [ 0.08657894  0.10395813  0.09859847  0.10723981  0.09893479  0.09873506\n",
      "   0.11645198  0.11309968  0.09176508  0.08463801]\n",
      " [ 0.0881099   0.10127839  0.09707937  0.10137521  0.10155913  0.10864381\n",
      "   0.09671022  0.10549483  0.10157643  0.09817272]\n",
      " [ 0.0820896   0.10546645  0.0910112   0.1075593   0.09938063  0.10440741\n",
      "   0.10204059  0.11057348  0.09658296  0.10088833]\n",
      " [ 0.07447707  0.08995915  0.08918563  0.11423235  0.11447746  0.10658044\n",
      "   0.10970163  0.11276966  0.10235683  0.08625984]\n",
      " [ 0.08162461  0.10576153  0.07705384  0.12049894  0.10831081  0.11936103\n",
      "   0.09947739  0.10753334  0.09767196  0.08270656]\n",
      " [ 0.08295083  0.11813262  0.09179574  0.10391767  0.10818919  0.09647456\n",
      "   0.09736709  0.10805692  0.09471323  0.09840213]\n",
      " [ 0.07309907  0.10282387  0.10308854  0.11249828  0.11195789  0.09589279\n",
      "   0.11256152  0.10521816  0.09649499  0.08636487]\n",
      " [ 0.07350772  0.11310808  0.08676659  0.10663546  0.10118837  0.09857303\n",
      "   0.11988107  0.12133333  0.09191252  0.08709376]\n",
      " [ 0.09512519  0.09778847  0.08604053  0.10838196  0.11186706  0.10371848\n",
      "   0.10314503  0.10281021  0.09601355  0.09510946]\n",
      " [ 0.07910684  0.08859913  0.10135671  0.09905319  0.10416432  0.09706938\n",
      "   0.10476933  0.12836181  0.09224378  0.1052756 ]\n",
      " [ 0.09414013  0.09718128  0.09607904  0.11457392  0.10052091  0.10414575\n",
      "   0.09176083  0.10337345  0.10828882  0.08993594]\n",
      " [ 0.08602411  0.1010969   0.09856621  0.11692864  0.08476847  0.10921627\n",
      "   0.10773654  0.10633856  0.10709818  0.08222611]\n",
      " [ 0.07874797  0.10420211  0.09614819  0.09667398  0.10733794  0.0900033\n",
      "   0.12054706  0.12177951  0.09567223  0.08888765]\n",
      " [ 0.08198317  0.09701365  0.1061057   0.12231467  0.10451621  0.10361346\n",
      "   0.09997138  0.09368352  0.10047136  0.09032691]\n",
      " [ 0.08403059  0.09506634  0.10219835  0.11220635  0.0956992   0.09632528\n",
      "   0.10290721  0.12489866  0.09588399  0.09078406]\n",
      " [ 0.09303477  0.10305637  0.09102955  0.09798002  0.09222976  0.11080861\n",
      "   0.12151731  0.10057357  0.09636246  0.09340757]\n",
      " [ 0.07935456  0.10619457  0.1124249   0.10959433  0.1011314   0.09844849\n",
      "   0.10445063  0.10731143  0.09554797  0.08554174]\n",
      " [ 0.07758597  0.09235177  0.09804813  0.11254935  0.09372664  0.09988672\n",
      "   0.10491473  0.11639038  0.10324951  0.10129675]\n",
      " [ 0.07410496  0.10481855  0.09894501  0.12315222  0.10557365  0.11106297\n",
      "   0.09722011  0.10156729  0.0878646   0.09569067]\n",
      " [ 0.08065481  0.10301418  0.09562325  0.11192472  0.10276734  0.10459057\n",
      "   0.12768313  0.10891511  0.08881608  0.07601081]\n",
      " [ 0.08077753  0.10901429  0.08809149  0.11384062  0.106983    0.10556891\n",
      "   0.10981651  0.11074681  0.08952493  0.08563591]\n",
      " [ 0.0764382   0.11000242  0.09840138  0.11005829  0.1026582   0.0902499\n",
      "   0.11511423  0.11289884  0.104846    0.07933251]\n",
      " [ 0.07979626  0.11585057  0.10869785  0.11430176  0.09967769  0.09483239\n",
      "   0.10864763  0.10434292  0.09196231  0.08189065]\n",
      " [ 0.09123767  0.0968449   0.08792467  0.11268254  0.11168775  0.09834881\n",
      "   0.10658611  0.1013784   0.09561421  0.09769494]\n",
      " [ 0.08502083  0.08867173  0.08897305  0.11638359  0.10831894  0.09959537\n",
      "   0.11910251  0.10453028  0.1015003   0.08790334]\n",
      " [ 0.08170677  0.1010748   0.09115919  0.10248674  0.10695689  0.10717057\n",
      "   0.10663714  0.10513972  0.10948302  0.08818515]\n",
      " [ 0.08404386  0.11066331  0.09731366  0.10570058  0.09815289  0.11971202\n",
      "   0.10328568  0.09828615  0.10219461  0.08064722]\n",
      " [ 0.08568266  0.08641825  0.09447245  0.11447974  0.09643868  0.097688\n",
      "   0.11282105  0.1142723   0.10222407  0.09550279]\n",
      " [ 0.09400336  0.09157018  0.09760708  0.11276257  0.10894512  0.10661423\n",
      "   0.09097373  0.10518882  0.09907298  0.09326199]\n",
      " [ 0.09096721  0.09785471  0.0862245   0.10526855  0.10603809  0.10887437\n",
      "   0.10810498  0.10024398  0.09935898  0.09706461]\n",
      " [ 0.07548532  0.08743612  0.10809889  0.11428153  0.09237375  0.11258267\n",
      "   0.10374174  0.12427438  0.10541256  0.07631304]\n",
      " [ 0.07168328  0.10507004  0.09970061  0.10533804  0.09383768  0.10141219\n",
      "   0.11505143  0.09941091  0.11116925  0.09732654]\n",
      " [ 0.09229708  0.10152978  0.09222025  0.0934203   0.10621312  0.10428565\n",
      "   0.10734473  0.11671861  0.09981111  0.0861594 ]\n",
      " [ 0.09405716  0.10342486  0.08688696  0.10792892  0.10077518  0.09844282\n",
      "   0.0958424   0.11378302  0.10098285  0.09787577]\n",
      " [ 0.08842403  0.10516004  0.08596063  0.10306264  0.10029535  0.1088815\n",
      "   0.10375686  0.11811572  0.10276742  0.08357581]\n",
      " [ 0.08233847  0.10077776  0.09447841  0.11296847  0.09509036  0.10782488\n",
      "   0.11771393  0.11723119  0.07916638  0.09241017]\n",
      " [ 0.09247311  0.09631511  0.08497781  0.11095915  0.11135845  0.1152166\n",
      "   0.09866746  0.10510226  0.09926016  0.08566987]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
